Project Overview:
The Sales Data Pipeline Project demonstrates a real-world data engineering pipeline built with Apache Spark and Delta Lake. This project simulates a typical ETL (Extract, Transform, Load) process, where raw sales data is ingested, cleaned, aggregated, and saved as both Delta Tables and CSV files for downstream analysis.
The pipeline showcases essential data transformation and data aggregation techniques required for handling structured data, focusing on scalability and real-time data processing.

________________________________________

Project Objectives:
â€¢	Ingest raw sales data from a CSV file.
â€¢	Clean and transform the data (convert string values to appropriate data types).
â€¢	Perform aggregations to calculate total sales per product.
â€¢	Save the cleaned and aggregated data as Delta Tables and CSV files for reporting.
â€¢	Handle large-scale data processing using Apache Spark

________________________________________

 Project Directory Structure:

 ![image](https://github.com/user-attachments/assets/fc36c17f-99a3-4cce-ac08-5373d3d4df74)




Technologies Used:
â€¢	Apache Spark (PySpark): For distributed data processing.
â€¢	Delta Lake: For handling Delta Tables and incremental data updates.
â€¢	Python: For scripting the ETL pipeline.
â€¢	Git: For version control.
â€¢	GitHub: For project hosting.

________________________________________

Data Transformation Process:
1.	Raw Data Ingestion:
o	The pipeline reads the raw sales.csv file containing sales records.

2.	Data Cleaning:
o	Converts string values (like Amount) to appropriate data types (e.g., float).
o	Filters out invalid or missing data.

3.	Aggregation:
o	Calculates the total sales per product and generates a summary dataset.
o	Saves the cleaned and aggregated data to both Delta Tables and CSV files.

________________________________________

Example Output:

Cleaned Sales Data:

![image](https://github.com/user-attachments/assets/53dce715-bda5-4103-b5ea-b4310164d72a)


Aggregated Sales Data:

![image](https://github.com/user-attachments/assets/4c3d0c1f-81ce-4c33-8dc0-562faeeb873f)

 
________________________________________

How to Run the Project Locally:

Step 1: Clone the Repository
git clone https://github.com/Anudeep-ui-tech/Sales-Data-Pipeline.git
cd Sales-Data-Pipeline 

Step 2: Install Dependencies
Make sure you have Apache Spark and Delta Lake configured in your local environment.

Step 3: Run the ETL Pipeline
python data_pipeline.py
 
Key Features:
â€¢	End-to-End ETL Pipeline
â€¢	Data Cleaning and Transformation
â€¢	Data Aggregation
â€¢	Delta Lake Integration
â€¢	Scalable and Efficient


________________________________________

Learning Outcomes:
By completing this project, I have gained hands-on experience with:
ðŸ”¹ Building an end-to-end data pipeline using Apache Spark.
ðŸ”¹ Using Delta Lake for efficient data storage and querying.
ðŸ”¹ Understanding the ETL process for real-world datasets.
ðŸ”¹ Handling large-scale data processing with PySpark.

ðŸ“© Contact Me:
If you have any questions or suggestions, feel free to reach out!
ðŸ“§ Email: [anudeep.pasala06@gmail.com]
ðŸ”— GitHub: Anudeep-ui-tech



